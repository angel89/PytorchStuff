# -*- coding: utf-8 -*-
"""DUDL_regular_minibatch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N_RpquDVPqGGhYv2yvOWHO0BT5pIt1sj

# COURSE: A deep understanding of deep learning
## SECTION: Regularization
### LECTURE: Batch training in action
#### TEACHER: Mike X Cohen, sincxpress.com
##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401
"""

# import libraries
import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader,TensorDataset

# for dataset management
import pandas as pd

import seaborn as sns

url = "https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv"

data = pd.read_csv(url,sep=';')
data

# describe the data
data.describe()

# list number of unique values per column
for i in data.keys():
  print(f'{i} has {len(np.unique(data[i]))} unique values')

# pairwise plots
cols2plot = ['fixed acidity','volatile acidity','citric acid','quality']
sns.pairplot(data[cols2plot],kind='reg',hue='quality')
plt.show()

### z-score all variables except for quality

# find the columns we want to normalize (all except quality)
cols2zscore = data.keys()
cols2zscore = cols2zscore.drop('quality')

# z-score (written out for clarity)
for col in cols2zscore:
  meanval   = np.mean(data[col])
  stdev     = np.std(data[col],ddof=1)
  data[col] = (data[col]-meanval) / stdev

# can also do more compactly
#data[cols2zscore] = data[cols2zscore].apply(stats.zscore)

data.describe()

fig,ax = plt.subplots(1,figsize=(17,4))
ax = sns.boxplot(data=data)
ax.set_xticklabels(ax.get_xticklabels(),rotation=45)
plt.show()

# distribution quality values
fig = plt.figure(figsize=(5,3))
plt.rcParams.update({'font.size': 15}) # increase font size in the figure

counts = data['quality'].value_counts()
plt.bar(list(counts.keys()),counts)
plt.xlabel('Quality rating')
plt.ylabel('Count')
plt.show()

# create a new column for binarized (boolean) quality
data['boolQuality'] = 0
# data['boolQuality'][data['quality']<6] = 0 # implicit in the code! just here for clarity
data['boolQuality'][data['quality']>5] = 1

data[['quality','boolQuality']]

# convert from pandas dataframe to tensor
dataT  = torch.tensor( data[cols2zscore].values ).float()
labels = torch.tensor( data['boolQuality'].values ).float()

print( dataT.shape )
print( labels.shape )

# we'll actually need the labels to be a "tensor"
labels = labels[:,None]
print( labels.shape )

# use scikitlearn to split the data
train_data,test_data, train_labels,test_labels = train_test_split(dataT, labels, test_size=.1)


# then convert them into PyTorch Datasets (note: already converted to tensors)
train_data = TensorDataset(train_data,train_labels)
test_data  = TensorDataset(test_data,test_labels)


# translate into dataloader objects
test_loader  = DataLoader(test_data,batch_size=test_data.tensors[0].shape[0])

"""# Break the data into batches

# Construct the model and training plans
"""

# a function that creates the ANN model

def createANewModel():

  # model architecture
  ANNiris = nn.Sequential(
      nn.Linear(11,16),   # input layer
      nn.ReLU(),         # activation unit
      nn.BatchNorm1d(16), #layer norm
      nn.Linear(16,32),   # hidden layer
      nn.ReLU(),         # activation unit
      nn.BatchNorm1d(32), #layer norm
      nn.Linear(32,32),   # hidden layer
      nn.ReLU(),         # activation unit
      nn.Linear(32,1),   # output units
        )

  # loss function
  lossfun = nn.BCEWithLogitsLoss()

  # optimizer
  optimizer = torch.optim.SGD(ANNiris.parameters(),lr=.01)

  return ANNiris,lossfun,optimizer

# train the model

# global parameter
numepochs = 1000

def trainTheModel():

  # initialize accuracies as empties
  trainAcc = []
  testAcc  = []
  losses   = []

  # loop over epochs
  for epochi in range(numepochs):

    # loop over training data batches
    batchAcc  = []
    batchLoss = []
    for X,y in train_loader:

      # forward pass and loss
      yHat = ANNiris(X)
      loss = lossfun(yHat,y)

      # backprop
      optimizer.zero_grad()
      loss.backward()
      optimizer.step()

      # compute training accuracy just for this batch
      batchAcc.append( 100*torch.mean(((yHat > 0) == y).float()).item() )
      batchLoss.append(loss.item())
    # end of batch loop...

    # now that we've trained through the batches, get their average training accuracy
    trainAcc.append( np.mean(batchAcc) )
    losses.append( np.mean(batchLoss) )

    # test accuracy
    X,y = next(iter(test_loader)) # extract X,y from test dataloader

    testAcc.append( 100*torch.mean(((ANNiris(X) > 0)  == y).float()).item() )


  # function output
  return trainAcc,testAcc,losses

"""# Test it out"""

# test the model
train_loader = DataLoader(train_data,batch_size=32,shuffle=True,drop_last=True)

ANNiris,lossfun,optimizer = createANewModel()
trainAcc,testAcc,losses = trainTheModel()

# range of batch sizes to test
batchSizes = 2**np.arange(1, 7)

trainAccXbatchSize = np.zeros((numepochs,len(batchSizes)) )
testAccXbatchSize  = np.zeros((numepochs,len(batchSizes) ))
lossesXbatchSize   = np.zeros((numepochs,len(batchSizes) ))

# # #   # create/train the model in batches
for batchsize in range(len( batchSizes)):
  train_loader = DataLoader(train_data,batch_size=int(batchSizes[batchsize]),shuffle=True,drop_last=True)

  ANNiris,lossfun,optimizer = createANewModel()
  trainAcc,testAcc,losses = trainTheModel()
  trainAccXbatchSize[:,batchsize] = trainAcc
  testAccXbatchSize[:,batchsize] = testAcc
  lossesXbatchSize[:,batchsize] = losses

fix, ax = plt.subplots(1,2,figsize=(17,7))
ax[0].plot(trainAccXbatchSize)
ax[0].set_title('Train accuracy')
ax[1].plot(testAccXbatchSize)
ax[1].set_title('test accuracy')


for i in range(2):
  ax[i].legend(batchSizes)
  ax[i].set_xlabel('Epoch')
  ax[i].set_ylabel('Accuracy %')
  ax[i].set_ylim([50,101])
  ax[i].grid()
plt.show()



"""# Additional explorations"""

# 1) Is there a relationship between the test_size parameter in train_test_split(), the batchsize parameter in DataLoader,
#    and the length of test_data? Think of your answer first, then test it in code, by creating new dataloader objects
#    with varying test_size parameters.
#    Hint: You can use the code 'len(test_data.dataset.tensors[1])', which returns the length of the labels vector.
#
# 2) Let's say you didn't care about the three types of irises; you only want a model that labels a flower as setosa or
#    "other." What would you have to change in the data and in the model to make this work?
#
# 3) In the course section "More on data," you will learn that unbalanced designs can be problematic for DL models (an
#    unbalanced design means that there is an uneven distribution of samples in different categories). Does the
#    modification in #2 produce an unbalanced design? To find out, count the number of data labels that are 0 (setosa) or
#    1 (not setosa).
#

